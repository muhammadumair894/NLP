{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","file_extension":".py","version":"3.6.3","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is it?\n\nWhen performing text vectorization, one common approach is to use a vector of word counts. Instead of weighting each word by its count within the document, though, another approach that’s often more helpful is to weight each word by its **tf-idf (term frequency-inverse document frequency)** score, defined as follows:\n\n```\ntf(t, d) = # of times term t appears in document d\nidf(t, D) = log (total number of documents / total # of documents in which term t appears)\ntf-idf(t, d, D) = tf(t, d) * idf(t, D)\n```\n\nFor example, given the following corpus:\n\n```\nDocument 1: The quick brown fox\nDocument 2: The brown brown dog\nDocument 3: The fox ate the dog\n```\n\nThe tf-idf score of “brown” within Document 2 is:\n\n```\ntf(brown, Document 2) = 2\nidf(brown, corpus) = log (3 / 2)\ntf-idf(brown, Document 2, corpus) = 2 * log (3 / 2)\n```\n\n# Why is it important?\n\nWeighting terms by the number of times they appear in a document is often suboptimal. For example, words like “the” can appear frequently within a document, but are relatively meaningless. Tf-idf scores help mitigate this problem by also taking into account a term’s frequency across the entire corpus, not just within the document itself.\n","metadata":{"_uuid":"64cb567bbc5c83f743243304579283e8f9250f68","_cell_guid":"8df28e82-732e-4a11-9e56-93e50075fc35"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\n\ncorpus = [\n 'the brown fox jumped over the brown dog',\n 'the quick brown fox',\n 'the brown brown dog',\n 'the fox ate the dog'\n]\n\nX = vectorizer.fit_transform(corpus)\n\nprint(vectorizer.get_feature_names())\nprint(X.toarray())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]}]}